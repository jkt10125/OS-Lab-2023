File: nmt-master/nmt/utils/evaluation_utils_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Tests for evaluation_utils.py."""

File: nmt-master/nmt/utils/misc_utils_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Tests for vocab_utils."""

File: nmt-master/nmt/utils/common_test_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Common utility functions for tests."""
40: """Create training and inference test hparams."""
43: # TODO(rzhao): Put num_residual_layers computation logic into
44: # `model_utils.py`, so we can also test it here.
49: # Networks
59: # Attention mechanisms
63: # Train
68: # Infer
73: # Misc
78: # Vocab
88: # For inference.py test
99: """Create test iterator."""

File: nmt-master/nmt/utils/standard_hparams_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """standard hparams utils."""
27: # Data
37: # Networks
50: # Attention mechanisms
56: # Train
70: # Data constraints
78: # Data format
85: # Misc
88: epoch_step=0,  # record where we were within an epoch.
95: # only enable beam search during inference when beam_width > 0.
103: # For inference
110: # Language model

File: nmt-master/nmt/utils/vocab_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Utility to handle vocabularies."""
30: # word level special token
36: # char ids 0-255 come from utf-8 encoding bytes
37: # assign 256-300 to special chars
38: BOS_CHAR_ID = 256  # <begin sentence>
39: EOS_CHAR_ID = 257  # <end sentence>
40: BOW_CHAR_ID = 258  # <begin word>
41: EOW_CHAR_ID = 259  # <end word>
42: PAD_CHAR_ID = 260  # <padding>
44: DEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.
48: """Given string and length, convert to byte seq of at most max_length.
    
    This process mimics docqa/elmo's preprocessing:
    https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py
    
    Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py &
    our usage differs from docqa/elmo.
    
    Args:
    text: tf.string tensor of shape []
    max_length: max number of chars for each word.
    
    Returns:
    A tf.int32 tensor of the byte encoded text.
    """
76: """Given a sequence of strings, map to sequence of bytes.
    
    Args:
    tokens: A tf.string tensor
    
    Returns:
    A tensor of shape words.shape + [bytes_per_word] containing byte versions
    of each word.
    """
113: """Check if vocab_file doesn't exist, create from corpus_file."""
115: utils.print_out("# Vocab file %s exists" % vocab_file)
118: # Verify if the vocab starts with unk, sos, eos
119: # If not, prepend those tokens & generate a new vocab file
144: """Creates vocab tables for src_vocab_file and tgt_vocab_file."""
156: """Load embed_file into a python dictionary.
    
    Note: the embed_file should be a Glove/word2vec formatted txt file. Assuming
    Here is an exampe assuming embed_size=5:
    
    the -0.071549 0.093459 0.023738 -0.090339 0.056123
    to 0.57346 0.5417 -0.23477 -0.3624 0.4037
    and 0.20327 0.47348 0.050877 0.002103 0.060547
    
    For word2vec format, the first line will be: <num_words> <emb_size>.
    
    Args:
    embed_file: file path to the embedding file.
    Returns:
    a dictionary that maps word to vector, and the size of embedding dimensions.
    """
181: if len(tokens) == 2:  # header line

File: nmt-master/nmt/utils/misc_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Generally useful utility functions."""
34: # LINT.IfChange
36: # LINT.ThenChange(<pwd>/nmt/copy.bara.sky)
43: """Exponentiation with catching of overflow error."""
52: """Take a start time, print elapsed duration, and return a new time."""
59: """Similar to print but with support to flush and output to a file."""
68: # stdout
80: """Print hparams, can skip keys based on pattern."""
90: """Load hparams from an existing model directory."""
93: print_out("# Loading hparams from %s" % hparams_file)
107: """Override hparams values with existing standard hparams config."""
109: print_out("# Loading standard hparams from %s" % hparams_path)
116: """Save hparams."""
124: """Print the shape and value of a tensor at test time. Return a new tensor."""
131: """Add a new summary to the current summary_writer.
    Useful to log things that are not part of the training graph, e.g., tag=BLEU.
    """
140: # GPU options:
141: # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html
147: # CPU threads options
157: """Convert a sequence words into sentence."""
158: if (not hasattr(words, "__len__") and  # for numpy array
165: """Convert a sequence of bpe words into sentence."""
174: else:  # end of a word
182: """Decode a text in SPM (https://github.com/google/sentencepiece) format."""

File: nmt-master/nmt/utils/iterator_utils_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Tests for iterator_utils.py"""
77: [[2, 0, 3],   # c a eos -- eos is padding
78: [-1, -1, 0]], # "f" == unknown, "e" == unknown, a
82: [[4, 1, 2],   # sos b c
83: [4, 2, 2]],  # sos c c
86: [[1, 2, 3],   # b c eos
87: [2, 2, 3]],  # c c eos
95: [[2, 2, 0]],  # c c a
99: [[4, 0, 1]],  # sos a b
102: [[0, 1, 3]],  # a b eos
157: [[2, 0, 3],     # c a eos -- eos is padding
158: [-1, -1, 0]],  # "f" == unknown, "e" == unknown, a
162: [[4, 1, 2],   # sos b c
163: [4, 2, 2]],  # sos c c
166: [[1, 2, 3],   # b c eos
167: [2, 2, 3]],  # c c eos
222: [[-1, -1, 0]], # "f" == unknown, "e" == unknown, a
226: [[4, 2, 2]],   # sos c c
229: [[2, 2, 3]],   # c c eos
236: # Re-init iterator with skip_count=0.
243: [[-1, -1, 0],  # "f" == unknown, "e" == unknown, a
244: [2, 0, 3]],   # c a eos -- eos is padding
248: [[4, 2, 2],   # sos c c
249: [4, 1, 2]],  # sos b c
252: [[2, 2, 3],   # c c eos
253: [1, 2, 3]],  # b c eos
261: [[2, 2, 0]],  # c c a
265: [[4, 0, 1]],  # sos a b
268: [[0, 1, 3]],  # a b eos
304: [[2, 2, 0],   # c c a
305: [2, 0, 3]],  # c a eos
311: [[-1, 3, 3],    # "d" == unknown, eos eos
312: [-1, -1, 0]],  # "f" == unknown, "e" == unknown, a

File: nmt-master/nmt/utils/__init__.py

File: nmt-master/nmt/utils/nmt_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Utility functions specifically for NMT."""
42: """Decode a test set and compute a score according to the evaluation task."""
43: # Decode
51: trans_f.write("")  # Write empty string to ensure file is created.
81: # Evaluation
97: """Given batch decoding outputs, select a sentence and turn to text."""
99: # Select a sentence
102: # If there is an eos symbol in outputs, cut them at that point.
106: if subword_option == "bpe":  # BPE
108: elif subword_option == "spm":  # SPM

File: nmt-master/nmt/utils/iterator_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
15: """For loading data into NMT models."""
28: # NOTE(ebrevdo): When we subclass this, instances' __dict__ becomes empty.
53: # Convert the word strings to character ids
57: # Convert the word strings to ids
61: # Add in the word counts.
73: # The entry is the source line rows;
74: # this has unknown-length vectors.  The last entry is
75: # the source row size; this is a scalar.
77: tf.TensorShape([None]),  # src
78: tf.TensorShape([])),  # src_len
79: # Pad the source sequences with eos tokens.
80: # (Though notice we don't generally need to do this since
81: # later on we will be masking out calculations past the true sequence.
83: src_eos_id,  # src
84: 0))  # src_len -- unused
141: # Filter zero length input sequences.
154: # Convert the word strings to ids.  Word strings that are not in the
155: # vocab get the lookup table's default_value integer.
168: # Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.
174: # Add in sequence lengths.
190: # Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)
194: # The first three entries are the source and target line rows;
195: # these have unknown-length vectors.  The last two entries are
196: # the source and target row sizes; these are scalars.
198: tf.TensorShape([None]),  # src
199: tf.TensorShape([None]),  # tgt_input
200: tf.TensorShape([None]),  # tgt_output
201: tf.TensorShape([]),  # src_len
202: tf.TensorShape([])),  # tgt_len
203: # Pad the source and target sequences with eos tokens.
204: # (Though notice we don't generally need to do this since
205: # later on we will be masking out calculations past the true sequence.
207: src_eos_id,  # src
208: tgt_eos_id,  # tgt_input
209: tgt_eos_id,  # tgt_output
210: 0,  # src_len -- unused
211: 0))  # tgt_len -- unused
216: # Calculate bucket_width by maximum source sequence length.
217: # Pairs with length [0, bucket_width) go to bucket 0, length
218: # [bucket_width, 2 * bucket_width) go to bucket 1, etc.  Pairs with length
219: # over ((num_bucket-1) * bucket_width) words all go into the last bucket.
225: # Bucket sentence pairs by the length of their source sentence and target
226: # sentence.

File: nmt-master/nmt/utils/vocab_utils_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Tests for vocab_utils."""
32: # Create a vocab file
41: # Call vocab_utils
47: # Assert: we expect the code to add  <unk>, <s>, </s> and
48: # create a new vocab file

File: nmt-master/nmt/utils/evaluation_utils.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Utility for evaluating various tasks, e.g., translation & summarization."""
32: """Pick a metric and evaluate depending on task."""
33: # BLEU scores for translation task
37: # ROUGE scores for summarization tasks
52: """Clean and handle BPE or SPM outputs."""
55: # BPE
59: # SPM
66: # Follow //transconsole/localization/machine_translation/metrics/bleu_calc.py
68: """Compute BLEU scores and handling BPE."""
93: # bleu_score, precisions, bp, ratio, translation_length, reference_length
100: """Compute ROUGE scores and handling BPE."""
118: """Compute accuracy, each line contains a label."""
134: """Compute accuracy on per word basis."""
154: """Compute BLEU scores using Moses multi-bleu.perl script."""
156: # TODO(thangluong): perform rewrite using python
157: # BPE
161: # TODO(thangluong): not use shell=True, can be a security hazard
176: # subprocess
177: # TODO(thangluong): not use shell=True, can be a security hazard
180: # extract BLEU score

File: nmt-master/nmt/model.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Basic sequence-to-sequence model with dynamic RNN support."""
41: """To allow for flexibily in returing different outputs."""
47: """To allow for flexibily in returing different outputs."""
54: """To allow for flexibily in returing different outputs."""
59: """Sequence-to-sequence base class.
    """
71: """Create the model.
    
    Args:
    hparams: Hyperparameter configurations.
    mode: TRAIN | EVAL | INFER
    iterator: Dataset Iterator that feeds data.
    source_vocab_table: Lookup table mapping source words to ids.
    target_vocab_table: Lookup table mapping target words to ids.
    reverse_target_vocab_table: Lookup table mapping ids to target words. Only
    required in INFER mode. Defaults to None.
    scope: scope of the model.
    extra_args: model_helper.ExtraArgs, for passing customizable functions.
    
    """
85: # Set params
90: # Not used in general seq2seq models; when True, ignore decoder & training
94: # Train graph
99: # Saver
111: """Set various params for self and initialize."""
130: # extra_args: to make it flexible for adding external customizable code
135: # Set num units
138: # Set num layers
144: # Set num residual layers
145: if hasattr(hparams, "num_residual_layers"):  # compatible common_test_utils
152: # Batch size
155: # Global step
158: # Initializer
164: # Embeddings
172: """Set up training and inference."""
186: ## Count the number of predicted words for compute ppl.
192: # Gradients and SGD update operation for training the model.
193: # Arrange for the embedding vars to appear at the beginning.
196: # warm-up
198: # decay
201: # Optimizer
209: # Gradients
223: # Summary
228: # Print trainable variables
229: utils.print_out("# Trainable variables")
236: """Get learning rate warmup."""
242: # Apply inverse decay if global steps less than warmup steps.
243: # Inspired by https://arxiv.org/pdf/1706.03762.pdf (Section 5.3)
244: # When step < warmup_steps,
245: #   learing_rate *= warmup_factor ** (warmup_steps - step)
247: # 0.01^(1/warmup_steps): we start with a lr, 100 times smaller
261: """Return decay info based on decay_scheme."""
275: elif not hparams.decay_scheme:  # no decay
284: """Get learning rate decay."""
302: """Init embeddings."""
320: """Get train summary."""
328: """Execute train graph."""
341: """Execute eval graph."""
349: """Subclass must implement this method.
    
    Creates a sequence-to-sequence model with dynamic RNN decoder API.
    Args:
    hparams: Hyperparameter configurations.
    scope: VariableScope for the created subgraph; default "dynamic_seq2seq".
    
    Returns:
    A tuple of the form (logits, loss_tuple, final_context_state, sample_id),
    where:
    logits: float32 Tensor [batch_size x num_decoder_symbols].
    loss: loss = the total loss / batch_size.
    final_context_state: the final state of decoder RNN.
    sample_id: sampling indices.
    
    Raises:
    ValueError: if encoder_type differs from mono and bi, or
    attention_option is not (luong | scaled_luong |
    bahdanau | normed_bahdanau).
    """
369: utils.print_out("# Creating %s graph ..." % self.mode)
371: # Projection
379: # Encoder
380: if hparams.language_model:  # no encoder for language modeling
387: # Skip decoder if extracting only encoder layers
391: ## Decoder
395: ## Loss
407: """Subclass must implement this.
    
    Build and run an RNN encoder.
    
    Args:
    hparams: Hyperparameters configurations.
    
    Returns:
    A tuple of encoder_outputs and encoder_state.
    """
421: """Build a multi-layer RNN cell that can be used by encoder."""
436: """Maximum decoding steps at inference time."""
441: # TODO(thangluong): add decoding_length_factor flag
449: """Build and run a RNN decoder with a final projection layer.
    
    Args:
    encoder_outputs: The outputs of encoder for every time step.
    encoder_state: The final state of the encoder.
    hparams: The Hyperparameters configurations.
    
    Returns:
    A tuple of final logits and final decoder state:
    logits: size [time, batch_size, vocab_size] when time_major=True.
    """
466: # maximum_iteration: The maximum decoding steps.
470: ## Decoder.
476: # Optional ops depends on which mode we are in and which loss function we
477: # are using.
481: ## Train or eval
483: # decoder_emp_inp: [max_time, batch_size, num_units]
490: # Helper
495: # Decoder
501: # Dynamic decoding
511: # Note: this is required when using sampled_softmax_loss.
514: # Note: there's a subtle difference here between train and inference.
515: # We could have set output_layer when create my_decoder
516: #   and shared more code between train and inference.
517: # We chose to apply the output_layer to all timesteps for speed:
518: #   10% improvements for small models & 20% for larger ones.
519: # If memory is a concern, we should apply output_layer per timestep.
523: # Colocate output layer with the last RNN cell if there is no extra GPU
524: # available. Otherwise, put last layer on a separate GPU.
529: logits = tf.no_op()  # unused when using sampled softmax loss.
531: ## Inference
558: # Helper
578: output_layer=self.output_layer  # applied per timestep
581: # Dynamic decoding
604: """Subclass must implement this.
    
    Args:
    hparams: Hyperparameters configurations.
    encoder_outputs: The outputs of encoder for every time step.
    encoder_state: The final state of the encoder.
    source_sequence_length: sequence length of encoder_outputs.
    
    Returns:
    A tuple of a multi-layer RNN cell used by decoder and the intial state of
    the decoder RNN.
    """
620: """Compute softmax loss or sampled softmax loss."""
652: """Compute optimization loss."""
683: """Decode a batch.
    
    Args:
    sess: tensorflow session to use.
    
    Returns:
    A tuple consiting of outputs, infer_summary.
    outputs: of size [batch_size, time]
    """
696: # make sure outputs is of shape [batch_size, time] or [beam_width,
697: # batch_size, time] when using beam search.
701: # beam search output in [batch_size, time, beam_width] shape.
706: """Stack encoder states and return tensor [batch, length, layer, size]."""
714: # transform from [length, batch, ...] -> [batch, length, ...]
722: """Sequence-to-sequence dynamic model.
    
    This class implements a multi-layer recurrent neural network as encoder,
    and a multi-layer recurrent neural network decoder.
    """
728: """Build an encoder from a sequence.
    
    Args:
    hparams: hyperparameters.
    sequence: tensor with input sequence data.
    sequence_length: tensor with length of the input sequence.
    
    Returns:
    encoder_outputs: RNN encoder outputs.
    encoder_state: RNN encoder state.
    
    Raises:
    ValueError: if encoder_type is neither "uni" nor "bi".
    """
754: # Encoder_outputs: [max_time, batch_size, num_units]
786: # alternatively concat forward and backward states
789: encoder_state.append(bi_encoder_state[0][layer_id])  # forward
790: encoder_state.append(bi_encoder_state[1][layer_id])  # backward
795: # Use the top layer for now
801: """Build encoder from source."""
802: utils.print_out("# Build a basic encoder")
811: """Create and call biddirectional RNN cells.
    
    Args:
    num_residual_layers: Number of residual layers from top to bottom. For
    example, if `num_bi_layers=4` and `num_residual_layers=2`, the last 2 RNN
    layers in each RNN cell will be wrapped with `ResidualWrapper`.
    base_gpu: The gpu device id to use for the first forward RNN layer. The
    i-th forward RNN layer will use `(base_gpu + i) % num_gpus` as its
    device id. The `base_gpu` for backward RNN cell is `(base_gpu +
    num_bi_layers)`.
    
    Returns:
    The concatenated bidirectional output and the bidirectional RNN cell"s
    state.
    """
826: # Construct forward and backward cells
849: """Build an RNN cell that can be used by decoder."""
850: # We only make use of encoder_outputs in attention-based models
873: # For beam search, we need to replicate encoder infos beam_width times

File: nmt-master/nmt/nmt_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
15: """Tests for nmt.py, train.py and inference.py."""
32: """Update flags for basic training."""
51: """Test the training loop is functional with basic hparams."""
65: """Test the training loop is functional with basic hparams."""
80: """Test inference is function with basic hparams."""
87: # Train one step so we have a checkpoint.
93: # Update FLAGS for inference.

File: nmt-master/nmt/gnmt_model.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """GNMT attention sequence-to-sequence model with dynamic RNN support."""
32: """Sequence-to-sequence dynamic model with GNMT attention architecture.
    """
58: """Build a GNMT encoder."""
65: # Build GNMT encoder.
68: utils.print_out("# Build a GNMT encoder")
83: # Execute _build_bidirectional_rnn from Model class
90: num_bi_residual_layers=0,  # no residual connection
93: # Build unidirectional layers
101: # Pass all encoder states to the decoder
102: #   except the first bi-directional layer
110: """Build encoder layers all at once."""
129: # Use the top layer for now
136: """Run each of the encoder layer separately, not used in general seq2seq."""
172: """Build a RNN cell with GNMT attention architecture."""
173: # Standard attention
178: # GNMT attention
203: cell_list = model_helper._cell_list(  # pylint: disable=protected-access
216: # Only wrap the bottom layer with the attention mechanism.
219: # Only generate alignment in greedy INFER mode.
225: attention_layer_size=None,  # don't use attention layer.
262: """A MultiCell with GNMT attention style."""
265: """Creates a GNMTAttentionMultiCell.
    
    Args:
    attention_cell: An instance of AttentionWrapper.
    cells: A list of RNNCell wrapped with AttentionInputWrapper.
    use_new_attention: Whether to use the attention generated from current
    step bottom layer's output. Default is False.
    """
278: """Run the cell with bottom layer's attention copied to all upper layers."""
311: """Residual function that handles different inputs and outputs inner dims.
    
    Args:
    inputs: cell inputs, this is actual inputs concatenated with the attention
    vector.
    outputs: cell outputs
    
    Returns:
    outputs + actual inputs
    """

File: nmt-master/nmt/scripts/bleu.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Python implementation of BLEU and smooth-BLEU.
    
    This module provides a Python implementation of BLEU and smooth-BLEU.
    Smooth BLEU is computed following the method outlined in the paper:
    Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic
    evaluation metrics for machine translation. COLING 2004.
    """
29: """Extracts all n-grams upto a given maximum order from an input segment.
    
    Args:
    segment: text segment from which n-grams will be extracted.
    max_order: maximum length in tokens of the n-grams returned by this
    methods.
    
    Returns:
    The Counter containing all n-grams upto max_order in segment
    with a count of how many times each n-gram occurred.
    """
50: """Computes BLEU score of translated segments against one or more references.
    
    Args:
    reference_corpus: list of lists of references for each translation. Each
    reference should be tokenized into a list of tokens.
    translation_corpus: list of translations to score. Each translation
    should be tokenized into a list of tokens.
    max_order: Maximum n-gram order to use when computing BLEU score.
    smooth: Whether or not to apply Lin et al. 2004 smoothing.
    
    Returns:
    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
    precisions and brevity penalty.
    """

File: nmt-master/nmt/scripts/__init__.py

File: nmt-master/nmt/scripts/rouge.py
1: """ROUGE metric implementation.
    
    Copy from tf_seq2seq/seq2seq/metrics/rouge.py.
    This is a modified and slightly extended verison of
    https://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py.
    """
16: #pylint: disable=C0103
20: """Calcualtes n-grams.
    
    Args:
    n: which n-grams to calculate
    text: An array of tokens
    
    Returns:
    A set of n-grams
    """
38: """Splits multiple sentences into words and flattens the result"""
43: """Calculates word n-grams for multiple sentences.
    """
53: """
    Returns the length of the Longest Common Subsequence between sequences x
    and y.
    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence
    
    Args:
    x: sequence of words
    y: sequence of words
    
    Returns
    integer: Length of LCS between x and y
    """
71: """
    Computes the length of the longest common subsequence (lcs) between two
    strings. The implementation below uses a DP programming algorithm and runs
    in O(nm) time where n = len(x) and m = len(y).
    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence
    
    Args:
    x: collection of words
    y: collection of words
    
    Returns:
    Table of dictionary of coord and len lcs
    """
98: """
    Returns the Longest Subsequence between x and y.
    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence
    
    Args:
    x: sequence of words
    y: sequence of words
    
    Returns:
    sequence: LCS of x and y
    """
113: """private recon calculation"""
128: """
    Computes ROUGE-N of two text collections of sentences.
    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/
    papers/rouge-working-note-v1.3.1.pdf
    
    Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentences: The sentences from the referene set
    n: Size of ngram.  Defaults to 2.
    
    Returns:
    A tuple (f1, precision, recall) for ROUGE-N
    
    Raises:
    ValueError: raises exception if a param has len <= 0
    """
152: # Gets the overlapping ngrams between evaluated and reference
156: # Handle edge case. This isn't mathematically correct, but it's good enough
169: # return overlapping_count / reference_count
174: """
    Computes the LCS-based F-measure score
    Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/
    rouge-working-note-v1.3.1.pdf
    
    Args:
    llcs: Length of LCS
    m: number of words in reference summary
    n: number of words in candidate summary
    
    Returns:
    Float. LCS-based F-measure score
    """
197: """
    Computes ROUGE-L (sentence level) of two text collections of sentences.
    http://research.microsoft.com/en-us/um/people/cyl/download/papers/
    rouge-working-note-v1.3.1.pdf
    
    Calculated according to:
    R_lcs = LCS(X,Y)/m
    P_lcs = LCS(X,Y)/n
    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)
    
    where:
    X = reference summary
    Y = Candidate summary
    m = length of reference summary
    n = length of candidate summary
    
    Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentences: The sentences from the referene set
    
    Returns:
    A float: F_lcs
    
    Raises:
    ValueError: raises exception if a param has len <= 0
    """
234: """
    Returns LCS_u(r_i, C) which is the LCS score of the union longest common
    subsequence between reference sentence ri and candidate summary C. For example
    if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and
    c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is
    "w1 w2" and the longest common subsequence of r_i and c2 is "w1 w3 w5". The
    union longest common subsequence of r_i, c1, and c2 is "w1 w2 w3 w5" and
    LCS_u(r_i, C) = 4/5.
    
    Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentence: One of the sentences in the reference summaries
    
    Returns:
    float: LCS_u(r_i, C)
    
    ValueError:
    Raises exception if a param has len <= 0
    """
271: """
    Computes ROUGE-L (summary level) of two text collections of sentences.
    http://research.microsoft.com/en-us/um/people/cyl/download/papers/
    rouge-working-note-v1.3.1.pdf
    
    Calculated according to:
    R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m
    P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n
    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)
    
    where:
    SUM(i,u) = SUM from i through u
    u = number of sentences in reference summary
    C = Candidate summary made up of v sentences
    m = number of words in reference summary
    n = number of words in candidate summary
    
    Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentence: One of the sentences in the reference summaries
    
    Returns:
    A float: F_lcs
    
    Raises:
    ValueError: raises exception if a param has len <= 0
    """
301: # total number of words in reference sentences
304: # total number of words in evaluated sentences
315: """Calculates average rouge scores for a list of hypotheses and
    references"""
318: # Filter out hyps that are of 0 length
319: # hyps_and_refs = zip(hypotheses, references)
320: # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]
321: # hypotheses, references = zip(*hyps_and_refs)
323: # Calculate ROUGE-1 F1, precision, recall scores
329: # Calculate ROUGE-2 F1, precision, recall scores
335: # Calculate ROUGE-L F1, precision, recall scores

File: nmt-master/nmt/attention_model.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
15: """Attention-based sequence-to-sequence model with dynamic RNN support."""
29: """Sequence-to-sequence dynamic model with attention.
    
    This class implements a multi-layer recurrent neural network as encoder,
    and an attention-based decoder. This is the same as the model described in
    (Luong et al., EMNLP'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.
    This class also allows to use GRU cells in addition to LSTM cells with
    support for dropout.
    """
49: # Set attention_mechanism_fn
79: """Build a RNN cell with attention mechanism that can be used by decoder."""
80: # No Attention
95: # Ensure memory is batch-major
110: # Attention
125: # Only generate alignment in greedy INFER mode.
136: # TODO(thangluong): do we need num_layers, num_gpus?
157: """Create attention mechanism based on the attention_option."""
158: del mode  # unused
160: # Mechanism
186: """create attention image and attention summary."""
188: # Reshape to (batch, src_seq_len, tgt_seq_len,1)
191: # Scale to range [0, 255]

File: nmt-master/nmt/train.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
15: """For training NMT models."""
45: """Sample decode a random sentence from src_data."""
64: """Compute internal evaluation (perplexity) for both dev / test.
    
    Computes development and testing perplexities for given model.
    
    Args:
    eval_model: Evaluation model for which to compute perplexities.
    eval_sess: Evaluation TensorFlow session.
    model_dir: Directory from which to load evaluation model from.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    use_test_set: Computes testing perplexity if true; does not otherwise.
    Note that the development perplexity is always computed regardless of
    value of this parameter.
    dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    development evaluation.
    test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    testing evaluation.
    Returns:
    Pair containing development perplexity and testing perplexity, in this
    order.
    """
127: """Compute external evaluation for both dev / test.
    
    Computes development and testing external evaluation (e.g. bleu, rouge) for
    given model.
    
    Args:
    infer_model: Inference model for which to compute perplexities.
    infer_sess: Inference TensorFlow session.
    model_dir: Directory from which to load inference model from.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    use_test_set: Computes testing external evaluation if true; does not
    otherwise. Note that the development external evaluation is always
    computed regardless of value of this parameter.
    dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    development external evaluation.
    test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    testing external evaluation.
    Returns:
    Triple containing development scores, testing scores and the TensorFlow
    Variable for the global step number, in this order.
    """
203: """Creates an averaged checkpoint and run external eval with it."""
206: # Convert VariableName:0 to VariableName.
235: """Compute internal evaluation (perplexity) for both dev / test.
    
    Computes development and testing perplexities for given model.
    
    Args:
    model_dir: Directory from which to load models from.
    infer_model: Inference model for which to compute perplexities.
    infer_sess: Inference TensorFlow session.
    eval_model: Evaluation model for which to compute perplexities.
    eval_sess: Evaluation TensorFlow session.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    avg_ckpts: Whether to compute average external evaluation scores.
    dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    internal development evaluation.
    test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    internal testing evaluation.
    dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    external development evaluation.
    test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
    Can be used to pass in additional inputs necessary for running the
    external testing evaluation.
    Returns:
    Triple containing results summary, global step Tensorflow Variable and
    metrics in this order.
    """
320: """Wrapper for running sample_decode, internal_eval and external_eval.
    
    Args:
    model_dir: Directory from which to load models from.
    infer_model: Inference model for which to compute perplexities.
    infer_sess: Inference TensorFlow session.
    eval_model: Evaluation model for which to compute perplexities.
    eval_sess: Evaluation TensorFlow session.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    sample_src_data: sample of source data for sample decoding.
    sample_tgt_data: sample of target data for sample decoding.
    avg_ckpts: Whether to compute average external evaluation scores.
    Returns:
    Triple containing results summary, global step Tensorflow Variable and
    metrics in this order.
    """
345: """Initialize statistics that we want to accumulate."""
347: "predict_count": 0.0,  # word count on the target side
348: "word_count": 0.0,  # word counts for both source and target
349: "sequence_count": 0.0,  # number of training examples processed
354: """Update stats: write summary and accumulate statistics."""
357: # Update statistics
371: """Print all info at the current global step."""
381: """Add stuffs in info to summaries."""
389: """Update info and check for overflow."""
390: # Per-step info
396: # Per-predict info
400: # Check for overflow
413: """Misc tasks to do before training."""
422: utils.print_out("# Start step %d, lr %g, %s" %
425: # Initialize all of the iterators
427: utils.print_out("# Init train iterator, skipping %d elements" % skip_count)
436: """Get the right model class depending on configuration."""
451: """Train a translation model."""
463: # Create model
469: # Preload data for sample decoding.
478: # Log and output files
481: utils.print_out("# log_file=%s" % log_file, log_f)
483: # TensorFlow model
499: # Summary writer
503: # First evaluation
514: # This is the training loop.
518: ### Run a step ###
524: # Finished going through the training dataset.  Go to next epoch.
527: "# Finished an epoch, step %d. Perform external evaluation" %
543: # Process step_result, accumulate stats, and write summary
548: # Once in a while, we print statistics.
558: # Reset statistics
563: utils.print_out("# Save eval, global step %d" % global_step)
566: # Save checkpoint
572: # Evaluate on dev/test
582: # Save checkpoint
598: # Done training
608: print_step_info("# Final, ", global_step, info, result_summary, log_f)
609: utils.print_time("# Done training!", start_train_time)
613: utils.print_out("# Start evaluating saved best models.")
621: print_step_info("# Best %s, " % metric, best_global_step, info,
632: print_step_info("# Averaged Best %s, " % metric, best_global_step, info,
640: """Format results."""
654: """Summary of the current best results."""
663: """Computing perplexity."""
673: """Pick a sentence and decode."""
675: utils.print_out("  # %d" % decode_id)
686: # get the top translation.
698: # Summary
706: """External evaluation such as BLEU and ROUGE scores."""
714: utils.print_out("# External evaluation, global step %d" % global_step)
731: # Save on best metrics
741: # metric: larger is better

File: nmt-master/nmt/inference_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Tests for model inference."""
38: # Prepare
47: # Create check point
115: # There are 5 examples, make batch_size=3 makes job0 has 3 examples, job1
116: # has 2 examples, and job2 has 0 example. This helps testing some edge
117: # cases.
129: # Note: Need to start job 0 at the end; otherwise, it will block the testing
130: # thread.
161: # TODO(rzhao): Make infer indices support batch_size > 1.

File: nmt-master/nmt/model_helper.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """Utility functions for building models."""
39: # If a vocab size is greater than this value, put the embedding on cpu instead
44: """Create an initializer. init_weight is only for uniform."""
60: """Return a device string for multi-GPU setup."""
82: """Create train graph, model, and iterator."""
115: # Note: One can set model_device_fn to
116: # `tf.train.replica_device_setter(ps_tasks)` for distributed training.
144: """Create train graph, model, src/tgt file holders, and iterator."""
197: """Create inference model."""
238: """Decide on which device to place an embed matrix given its vocab size."""
248: """Load pretrain embeding from embed_file, and return an embedding matrix.
    
    Args:
    embed_file: Path to a Glove formated embedding txt file.
    num_trainable_tokens: Make the first n tokens in the vocab file as trainable
    variables. Default is 3, which is "<unk>", "<s>" and "</s>".
    """
258: utils.print_out("# Using pretrained embedding: %s." % embed_file)
280: """Create a new or load an existing embedding matrix."""
304: """Create embedding matrix for both encoder and decoder.
    
    Args:
    share_vocab: A boolean. Whether to share embedding matrix for both
    encoder and decoder.
    src_vocab_size: An integer. The source vocab size.
    tgt_vocab_size: An integer. The target vocab size.
    src_embed_size: An integer. The embedding dimension for the encoder's
    embedding.
    tgt_embed_size: An integer. The embedding dimension for the decoder's
    embedding.
    dtype: dtype of the embedding matrix. Default to float32.
    num_enc_partitions: number of partitions used for the encoder's embedding
    vars.
    num_dec_partitions: number of partitions used for the decoder's embedding
    vars.
    scope: VariableScope for the created subgraph. Default to "embedding".
    
    Returns:
    embedding_encoder: Encoder's embedding matrix.
    embedding_decoder: Decoder's embedding matrix.
    
    Raises:
    ValueError: if use share_vocab but source and target have different vocab
    size.
    """
333: # Note: num_partitions > 1 is required for distributed training due to
334: # embedding_lookup tries to colocate single partition-ed embedding variable
335: # with lookup ops. This may cause embedding variables being placed on worker
336: # jobs.
342: # Note: num_partitions > 1 is required for distributed training due to
343: # embedding_lookup tries to colocate single partition-ed embedding variable
344: # with lookup ops. This may cause embedding variables being placed on worker
345: # jobs.
360: # Share embedding
366: utils.print_out("# Use the same embedding for source and target")
393: """Create an instance of a single RNN cell."""
394: # dropout (= 1 - keep_prob) is set to 0 during eval and infer
397: # Cell Type
419: # Dropout (= 1 - keep_prob)
426: # Residual
432: # Device Wrapper
444: """Create a list of RNN cells."""
448: # Multi-GPU
471: """Create multi-layer RNN cell.
    
    Args:
    unit_type: string representing the unit type, i.e. "lstm".
    num_units: the depth of each unit.
    num_layers: number of cells.
    num_residual_layers: Number of residual layers from top to bottom. For
    example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN
    cells in the returned list will be wrapped with `ResidualWrapper`.
    forget_bias: the initial forget bias of the RNNCell(s).
    dropout: floating point value between 0.0 and 1.0:
    the probability of dropout.  this is ignored if `mode != TRAIN`.
    mode: either tf.contrib.learn.TRAIN/EVAL/INFER
    num_gpus: The number of gpus to use when performing round-robin
    placement of layers.
    base_gpu: The gpu device id to use for the first RNN cell in the
    returned list. The i-th RNN cell will use `(base_gpu + i) % num_gpus`
    as its device id.
    single_cell_fn: allow for adding customized cell.
    When not specified, we default to model_helper._single_cell
    Returns:
    An `RNNCell` instance.
    """
505: if len(cell_list) == 1:  # Single layer.
507: else:  # Multi layers
512: """Clipping gradients of a model."""
523: """Print a list of variables in a checkpoint together with their shapes."""
524: utils.print_out("# Variables in ckpt %s" % ckpt_path)
532: """Load model from a checkpoint."""
550: """Average the last N checkpoints in the model_dir."""
553: utils.print_out("# No checkpoint file found in directory: %s" % model_dir)
556: # Checkpoints are ordered from oldest to newest.
562: "# Skipping averaging checkpoints because not enough checkpoints is "
570: "# Creating new directory %s for saving averaged checkpoints." %
574: utils.print_out("# Reading and averaging variables in checkpoints:")
592: # Build a graph with same variables in the checkpoints, and save the averaged
593: # variables into the avg_model_dir.
612: # Use the built saver to save the averaged checkpoint. Only keep 1
613: # checkpoint and the best checkpoint will be moved to avg_best_metric_dir.
622: """Create translation model and initialize or load parameters in session."""
638: """Compute perplexity of the output of the model.
    
    Args:
    model: model for compute perplexity.
    sess: tensorflow session to use.
    name: name of the batch.
    
    Returns:
    The perplexity of the eval outputs.
    """

File: nmt-master/nmt/inference.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """To perform inference on test set given a trained model."""
40: """Decoding only a specific set of sentences."""
46: trans_f.write("")  # Write empty string to ensure file is created.
50: # get text translation
58: if infer_summary is not None:  # Attention models
72: """Load inference data."""
84: """Get the right model class depending on configuration."""
99: """Start session and load model."""
115: """Perform translation."""
150: """Inference with a single worker."""
153: # Read data
163: # Decode
164: utils.print_out("# Start decoding")
197: """Inference using multiple workers."""
204: # Read data
207: # Split data to multiple workers
220: # Decode
221: utils.print_out("# Start decoding")
235: # Change file name to indicate the file writing is completed.
238: # Job 0 is responsible for the clean up.
241: # Now write all translations

File: nmt-master/nmt/model_test.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
15: """Tests for model.py."""
360: ## Testing 3 encoders:
361: # uni: no attention, no residual, 1 layers
362: # bi: no attention, with residual, 4 layers
374: # pylint: disable=line-too-long
384: # pylint: enable=line-too-long
431: # pylint: disable=line-too-long
453: # pylint: enable=line-too-long
489: ## Test attention mechanisms: luong, scaled_luong, bahdanau, normed_bahdanau
501: # pylint: disable=line-too-long
517: # pylint: enable=line-too-long
529: # pylint: disable=line-too-long
536: # pylint: enable=line-too-long
569: # pylint: disable=line-too-long
586: # pylint: enable=line-too-long
598: # pylint: disable=line-too-long
605: # pylint: enable=line-too-long
643: # pylint: disable=line-too-long
661: # pylint: enable=line-too-long
673: # pylint: disable=line-too-long
680: # pylint: enable=line-too-long
713: # pylint: disable=line-too-long
733: # pylint: enable=line-too-long
746: # pylint: disable=line-too-long
753: # pylint: enable=line-too-long
780: ## Test encoder vs. attention (all use residual):
781: # uni encoder, standard attention
792: # pylint: disable=line-too-long
817: # pylint: enable=line-too-long
860: # Test gnmt model.
871: # pylint: disable=line-too-long
897: # pylint: enable=line-too-long
940: # Test beam search.

File: nmt-master/nmt/__init__.py

File: nmt-master/nmt/nmt.py
1: # Copyright 2017 Google Inc. All Rights Reserved.
2: #
3: # Licensed under the Apache License, Version 2.0 (the "License");
4: # you may not use this file except in compliance with the License.
5: # You may obtain a copy of the License at
6: #
7: #     http://www.apache.org/licenses/LICENSE-2.0
8: #
9: # Unless required by applicable law or agreed to in writing, software
10: # distributed under the License is distributed on an "AS IS" BASIS,
11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12: # See the License for the specific language governing permissions and
13: # limitations under the License.
14: # ==============================================================================
16: """TensorFlow NMT model implementation."""
24: # import matplotlib.image as mpimg
46: """Build ArgumentParser."""
49: # network
70: # attention mechanisms
95: # optimizer
120: # initializer
127: # data
141: # Vocab
157: # Sequence lengths
168: # Default settings works well (rarely need to change)
190: # SPM
195: # Experimental encoding feature.
200: """)
    
    # Misc
202: # Misc
    parser.add_argument("--num_gpus", type=int, default=1,
    help="Number of gpus in each worker.")
    parser.add_argument("--log_device_placement", type="bool", nargs="?",
    const=True, default=False, help="Debug GPU allocation.")
    parser.add_argument("--metrics", type=str, default="bleu",
    help=("Comma-separated list of evaluations "
    "metrics (bleu,rouge,accuracy)"))
    parser.add_argument("--steps_per_external_eval", type=int, default=None,
    help="""      How many training steps to do per external evaluation.  Automatically set
    based on data if None.      """)
    parser.add_argument("--scope", type=str, default=None,
    help="scope to put variables under")
    parser.add_argument("--hparams_path", type=str, default=None,
    help=("Path to standard hparams json file that overrides"
    "hparams values from FLAGS."))
    parser.add_argument("--random_seed", type=int, default=None,
    help="Random seed (>0, set a specific seed).")
    parser.add_argument("--override_loaded_hparams", type="bool", nargs="?",
    const=True, default=False,
    help="Override loaded hparams with values specified")
    parser.add_argument("--num_keep_ckpts", type=int, default=5,
    help="Max number of checkpoints to keep.")
    parser.add_argument("--avg_ckpts", type="bool", nargs="?",
    const=True, default=False, help=("""                      Average the last N checkpoints for external evaluation.
    N can be controlled by setting --num_keep_ckpts.                      """))
    parser.add_argument("--language_model", type="bool", nargs="?",
    const=True, default=False,
    help="True to train a language model, ignoring encoder")
    
    # Inference
232: # Inference
    parser.add_argument("--ckpt", type=str, default="",
    help="Checkpoint file to load a model for inference.")
    parser.add_argument("--inference_input_file", type=str, default=None,
    help="Set to the text to decode.")
    parser.add_argument("--inference_list", type=str, default=None,
    help=("A comma-separated list of sentence indices "
    "(0-based) to decode."))
    parser.add_argument("--infer_batch_size", type=int, default=32,
    help="Batch size for inference mode.")
    parser.add_argument("--inference_output_file", type=str, default=None,
    help="Output file to store decoding results.")
    parser.add_argument("--inference_ref_file", type=str, default=None,
    help=("""      Reference file to compute evaluation scores (if provided).      """))
    
    # Advanced inference arguments
247: # Advanced inference arguments
    parser.add_argument("--infer_mode", type=str, default="greedy",
    choices=["greedy", "sample", "beam_search"],
    help="Which type of decoder to use during inference.")
    parser.add_argument("--beam_width", type=int, default=0,
    help=("""      beam width when using beam search decoder. If 0 (default), use standard
    decoder with greedy helper.      """))
    parser.add_argument("--length_penalty_weight", type=float, default=0.0,
    help="Length penalty for beam search.")
    parser.add_argument("--coverage_penalty_weight", type=float, default=0.0,
    help="Coverage penalty for beam search.")
    parser.add_argument("--sampling_temperature", type=float,
    default=0.0,
    help=("""      Softmax sampling temperature for inference decoding, 0.0 means greedy
    decoding. This option is ignored when using beam search.      """))
    parser.add_argument("--num_translations_per_input", type=int, default=1,
    help=("""      Number of translations generated for each sentence. This is only used for
    inference.      """))
    
    # Job info
266: # Job info
    parser.add_argument("--jobid", type=int, default=0,
    help="Task id of the worker.")
    parser.add_argument("--num_workers", type=int, default=1,
    help="Number of workers (inference only).")
    parser.add_argument("--num_inter_threads", type=int, default=0,
    help="number of inter_op_parallelism_threads")
    parser.add_argument("--num_intra_threads", type=int, default=0,
    help="number of intra_op_parallelism_threads")
    
    
    def create_hparams(flags):
278: """Create training hparams."""
    return tf.contrib.training.HParams(
    # Data
280: # Data
    src=flags.src,
    tgt=flags.tgt,
    train_prefix=flags.train_prefix,
    dev_prefix=flags.dev_prefix,
    test_prefix=flags.test_prefix,
    vocab_prefix=flags.vocab_prefix,
    embed_prefix=flags.embed_prefix,
    out_dir=flags.out_dir,
    
    # Networks
290: # Networks
    num_units=flags.num_units,
    num_encoder_layers=(flags.num_encoder_layers or flags.num_layers),
    num_decoder_layers=(flags.num_decoder_layers or flags.num_layers),
    dropout=flags.dropout,
    unit_type=flags.unit_type,
    encoder_type=flags.encoder_type,
    residual=flags.residual,
    time_major=flags.time_major,
    num_embeddings_partitions=flags.num_embeddings_partitions,
    
    # Attention mechanisms
301: # Attention mechanisms
    attention=flags.attention,
    attention_architecture=flags.attention_architecture,
    output_attention=flags.output_attention,
    pass_hidden_state=flags.pass_hidden_state,
    
    # Train
307: # Train
    optimizer=flags.optimizer,
    num_train_steps=flags.num_train_steps,
    batch_size=flags.batch_size,
    init_op=flags.init_op,
    init_weight=flags.init_weight,
    max_gradient_norm=flags.max_gradient_norm,
    learning_rate=flags.learning_rate,
    warmup_steps=flags.warmup_steps,
    warmup_scheme=flags.warmup_scheme,
    decay_scheme=flags.decay_scheme,
    colocate_gradients_with_ops=flags.colocate_gradients_with_ops,
    num_sampled_softmax=flags.num_sampled_softmax,
    
    # Data constraints
321: # Data constraints
    num_buckets=flags.num_buckets,
    max_train=flags.max_train,
    src_max_len=flags.src_max_len,
    tgt_max_len=flags.tgt_max_len,
    
    # Inference
327: # Inference
    src_max_len_infer=flags.src_max_len_infer,
    tgt_max_len_infer=flags.tgt_max_len_infer,
    infer_batch_size=flags.infer_batch_size,
    
    # Advanced inference arguments
332: # Advanced inference arguments
    infer_mode=flags.infer_mode,
    beam_width=flags.beam_width,
    length_penalty_weight=flags.length_penalty_weight,
    coverage_penalty_weight=flags.coverage_penalty_weight,
    sampling_temperature=flags.sampling_temperature,
    num_translations_per_input=flags.num_translations_per_input,
    
    # Vocab
340: # Vocab
    sos=flags.sos if flags.sos else vocab_utils.SOS,
    eos=flags.eos if flags.eos else vocab_utils.EOS,
    subword_option=flags.subword_option,
    check_special_token=flags.check_special_token,
    use_char_encode=flags.use_char_encode,
    
    # Misc
347: # Misc
    forget_bias=flags.forget_bias,
    num_gpus=flags.num_gpus,
    epoch_step=0,  # record where we were within an epoch.
350: epoch_step=0,  # record where we were within an epoch.
    steps_per_stats=flags.steps_per_stats,
    steps_per_external_eval=flags.steps_per_external_eval,
    share_vocab=flags.share_vocab,
    metrics=flags.metrics.split(","),
    log_device_placement=flags.log_device_placement,
    random_seed=flags.random_seed,
    override_loaded_hparams=flags.override_loaded_hparams,
    num_keep_ckpts=flags.num_keep_ckpts,
    avg_ckpts=flags.avg_ckpts,
    language_model=flags.language_model,
    num_intra_threads=flags.num_intra_threads,
    num_inter_threads=flags.num_inter_threads,
    )
    
    
    def _add_argument(hparams, key, value, update=True):
367: """Add an argument to hparams; if exists, change the value if update==True."""
    if hasattr(hparams, key):
    if update:
    setattr(hparams, key, value)
    else:
    hparams.add_hparam(key, value)
    
    
    def extend_hparams(hparams):
376: """Add new arguments to hparams."""
    # Sanity checks
377: # Sanity checks
    if hparams.encoder_type == "bi" and hparams.num_encoder_layers % 2 != 0:
    raise ValueError("For bi, num_encoder_layers %d should be even" %
    hparams.num_encoder_layers)
    if (hparams.attention_architecture in ["gnmt"] and
    hparams.num_encoder_layers < 2):
    raise ValueError("For gnmt attention architecture, "
    "num_encoder_layers %d should be >= 2" %
    hparams.num_encoder_layers)
    if hparams.subword_option and hparams.subword_option not in ["spm", "bpe"]:
    raise ValueError("subword option must be either spm, or bpe")
    if hparams.infer_mode == "beam_search" and hparams.beam_width <= 0:
    raise ValueError("beam_width must greater than 0 when using beam_search"
    "decoder.")
    if hparams.infer_mode == "sample" and hparams.sampling_temperature <= 0.0:
    raise ValueError("sampling_temperature must greater than 0.0 when using"
    "sample decoder.")
    
    # Different number of encoder / decoder layers
395: # Different number of encoder / decoder layers
    assert hparams.num_encoder_layers and hparams.num_decoder_layers
    if hparams.num_encoder_layers != hparams.num_decoder_layers:
    hparams.pass_hidden_state = False
    utils.print_out("Num encoder layer %d is different from num decoder layer"
    " %d, so set pass_hidden_state to False" % (
    hparams.num_encoder_layers,
    hparams.num_decoder_layers))
    
    # Set residual layers
404: # Set residual layers
    num_encoder_residual_layers = 0
    num_decoder_residual_layers = 0
    if hparams.residual:
    if hparams.num_encoder_layers > 1:
    num_encoder_residual_layers = hparams.num_encoder_layers - 1
    if hparams.num_decoder_layers > 1:
    num_decoder_residual_layers = hparams.num_decoder_layers - 1
    
    if hparams.encoder_type == "gnmt":
    # The first unidirectional layer (after the bi-directional layer) in
414: # The first unidirectional layer (after the bi-directional layer) in
    # the GNMT encoder can't have residual connection due to the input is
415: # the GNMT encoder can't have residual connection due to the input is
    # the concatenation of fw_cell and bw_cell's outputs.
416: # the concatenation of fw_cell and bw_cell's outputs.
    num_encoder_residual_layers = hparams.num_encoder_layers - 2
    
    # Compatible for GNMT models
419: # Compatible for GNMT models
    if hparams.num_encoder_layers == hparams.num_decoder_layers:
    num_decoder_residual_layers = num_encoder_residual_layers
    _add_argument(hparams, "num_encoder_residual_layers",
    num_encoder_residual_layers)
    _add_argument(hparams, "num_decoder_residual_layers",
    num_decoder_residual_layers)
    
    # Language modeling
427: # Language modeling
    if getattr(hparams, "language_model", None):
    hparams.attention = ""
    hparams.attention_architecture = ""
    hparams.pass_hidden_state = False
    hparams.share_vocab = True
    hparams.src = hparams.tgt
    utils.print_out("For language modeling, we turn off attention and "
    "pass_hidden_state; turn on share_vocab; set src to tgt.")
    
    ## Vocab
437: ## Vocab
    # Get vocab file names first
438: # Get vocab file names first
    if hparams.vocab_prefix:
    src_vocab_file = hparams.vocab_prefix + "." + hparams.src
    tgt_vocab_file = hparams.vocab_prefix + "." + hparams.tgt
    else:
    raise ValueError("hparams.vocab_prefix must be provided.")
    
    # Source vocab
445: # Source vocab
    check_special_token = getattr(hparams, "check_special_token", True)
    src_vocab_size, src_vocab_file = vocab_utils.check_vocab(
    src_vocab_file,
    hparams.out_dir,
    check_special_token=check_special_token,
    sos=hparams.sos,
    eos=hparams.eos,
    unk=vocab_utils.UNK)
    
    # Target vocab
455: # Target vocab
    if hparams.share_vocab:
    utils.print_out("  using source vocab for target")
    tgt_vocab_file = src_vocab_file
    tgt_vocab_size = src_vocab_size
    else:
    tgt_vocab_size, tgt_vocab_file = vocab_utils.check_vocab(
    tgt_vocab_file,
    hparams.out_dir,
    check_special_token=check_special_token,
    sos=hparams.sos,
    eos=hparams.eos,
    unk=vocab_utils.UNK)
    _add_argument(hparams, "src_vocab_size", src_vocab_size)
    _add_argument(hparams, "tgt_vocab_size", tgt_vocab_size)
    _add_argument(hparams, "src_vocab_file", src_vocab_file)
    _add_argument(hparams, "tgt_vocab_file", tgt_vocab_file)
    
    # Num embedding partitions
473: # Num embedding partitions
    num_embeddings_partitions = getattr(hparams, "num_embeddings_partitions", 0)
    _add_argument(hparams, "num_enc_emb_partitions", num_embeddings_partitions)
    _add_argument(hparams, "num_dec_emb_partitions", num_embeddings_partitions)
    
    # Pretrained Embeddings
478: # Pretrained Embeddings
    _add_argument(hparams, "src_embed_file", "")
    _add_argument(hparams, "tgt_embed_file", "")
    if getattr(hparams, "embed_prefix", None):
    src_embed_file = hparams.embed_prefix + "." + hparams.src
    tgt_embed_file = hparams.embed_prefix + "." + hparams.tgt
    
    if tf.gfile.Exists(src_embed_file):
    utils.print_out("  src_embed_file %s exist" % src_embed_file)
    hparams.src_embed_file = src_embed_file
    
    utils.print_out(
    "For pretrained embeddings, set num_enc_emb_partitions to 1")
    hparams.num_enc_emb_partitions = 1
    else:
    utils.print_out("  src_embed_file %s doesn't exist" % src_embed_file)
    
    if tf.gfile.Exists(tgt_embed_file):
    utils.print_out("  tgt_embed_file %s exist" % tgt_embed_file)
    hparams.tgt_embed_file = tgt_embed_file
    
    utils.print_out(
    "For pretrained embeddings, set num_dec_emb_partitions to 1")
    hparams.num_dec_emb_partitions = 1
    else:
    utils.print_out("  tgt_embed_file %s doesn't exist" % tgt_embed_file)
    
    # Evaluation
505: # Evaluation
    for metric in hparams.metrics:
    best_metric_dir = os.path.join(hparams.out_dir, "best_" + metric)
    tf.gfile.MakeDirs(best_metric_dir)
    _add_argument(hparams, "best_" + metric, 0, update=False)
    _add_argument(hparams, "best_" + metric + "_dir", best_metric_dir)
    
    if getattr(hparams, "avg_ckpts", None):
    best_metric_dir = os.path.join(hparams.out_dir, "avg_best_" + metric)
    tf.gfile.MakeDirs(best_metric_dir)
    _add_argument(hparams, "avg_best_" + metric, 0, update=False)
    _add_argument(hparams, "avg_best_" + metric + "_dir", best_metric_dir)
    
    return hparams
    
    
    def ensure_compatible_hparams(hparams, default_hparams, hparams_path=""):
522: """Make sure the loaded hparams is compatible with new changes."""
    default_hparams = utils.maybe_parse_standard_hparams(
    default_hparams, hparams_path)
    
    # Set num encoder/decoder layers (for old checkpoints)
526: # Set num encoder/decoder layers (for old checkpoints)
    if hasattr(hparams, "num_layers"):
    if not hasattr(hparams, "num_encoder_layers"):
    hparams.add_hparam("num_encoder_layers", hparams.num_layers)
    if not hasattr(hparams, "num_decoder_layers"):
    hparams.add_hparam("num_decoder_layers", hparams.num_layers)
    
    # For compatible reason, if there are new fields in default_hparams,
533: # For compatible reason, if there are new fields in default_hparams,
    #   we add them to the current hparams
534: #   we add them to the current hparams
    default_config = default_hparams.values()
    config = hparams.values()
    for key in default_config:
    if key not in config:
    hparams.add_hparam(key, default_config[key])
    
    # Update all hparams' keys if override_loaded_hparams=True
541: # Update all hparams' keys if override_loaded_hparams=True
    if getattr(default_hparams, "override_loaded_hparams", None):
    overwritten_keys = default_config.keys()
    else:
    # For inference
545: # For inference
    overwritten_keys = INFERENCE_KEYS
    
    for key in overwritten_keys:
    if getattr(hparams, key) != default_config[key]:
    utils.print_out("# Updating hparams.%s: %s -> %s" %
550: utils.print_out("# Updating hparams.%s: %s -> %s" %
    (key, str(getattr(hparams, key)),
    str(default_config[key])))
    setattr(hparams, key, default_config[key])
    return hparams
    
    
    def create_or_load_hparams(
    out_dir, default_hparams, hparams_path, save_hparams=True):
559: """Create hparams or load hparams from out_dir."""
    hparams = utils.load_hparams(out_dir)
    if not hparams:
    hparams = default_hparams
    hparams = utils.maybe_parse_standard_hparams(
    hparams, hparams_path)
    else:
    hparams = ensure_compatible_hparams(hparams, default_hparams, hparams_path)
    hparams = extend_hparams(hparams)
    
    # Save HParams
569: # Save HParams
    if save_hparams:
    utils.save_hparams(out_dir, hparams)
    for metric in hparams.metrics:
    utils.save_hparams(getattr(hparams, "best_" + metric + "_dir"), hparams)
    
    # Print HParams
575: # Print HParams
    utils.print_hparams(hparams)
    return hparams
    
    
    def run_main(flags, default_hparams, train_fn, inference_fn, target_session=""):
581: """Run main."""
    # Job
582: # Job
    jobid = flags.jobid
    num_workers = flags.num_workers
    utils.print_out("# Job id %d" % jobid)
585: utils.print_out("# Job id %d" % jobid)
    
    # GPU device
587: # GPU device
    utils.print_out(
    "# Devices visible to TensorFlow: %s" % repr(tf.Session().list_devices()))
589: "# Devices visible to TensorFlow: %s" % repr(tf.Session().list_devices()))
    
    # Random
591: # Random
    random_seed = flags.random_seed
    if random_seed is not None and random_seed > 0:
    utils.print_out("# Set random seed to %d" % random_seed)
594: utils.print_out("# Set random seed to %d" % random_seed)
    random.seed(random_seed + jobid)
    np.random.seed(random_seed + jobid)
    
    # Model output directory
598: # Model output directory
    out_dir = flags.out_dir
    if out_dir and not tf.gfile.Exists(out_dir):
    utils.print_out("# Creating output directory %s ..." % out_dir)
601: utils.print_out("# Creating output directory %s ..." % out_dir)
    tf.gfile.MakeDirs(out_dir)
    
    # Load hparams.
604: # Load hparams.
    loaded_hparams = False
    if flags.ckpt:  # Try to load hparams from the same directory as ckpt
606: if flags.ckpt:  # Try to load hparams from the same directory as ckpt
    ckpt_dir = os.path.dirname(flags.ckpt)
    ckpt_hparams_file = os.path.join(ckpt_dir, "hparams")
    if tf.gfile.Exists(ckpt_hparams_file) or flags.hparams_path:
    hparams = create_or_load_hparams(
    ckpt_dir, default_hparams, flags.hparams_path,
    save_hparams=False)
    loaded_hparams = True
    if not loaded_hparams:  # Try to load from out_dir
614: if not loaded_hparams:  # Try to load from out_dir
    assert out_dir
    hparams = create_or_load_hparams(
    out_dir, default_hparams, flags.hparams_path,
    save_hparams=(jobid == 0))
    
    ## Train / Decode
620: ## Train / Decode
    if flags.inference_input_file:
    # Inference output directory
622: # Inference output directory
    trans_file = flags.inference_output_file
    assert trans_file
    trans_dir = os.path.dirname(trans_file)
    if not tf.gfile.Exists(trans_dir): tf.gfile.MakeDirs(trans_dir)
    
    # Inference indices
628: # Inference indices
    hparams.inference_indices = None
    if flags.inference_list:
    (hparams.inference_indices) = (
    [int(token)  for token in flags.inference_list.split(",")])
    
    # Inference
634: # Inference
    ckpt = flags.ckpt
    if not ckpt:
    ckpt = tf.train.latest_checkpoint(out_dir)
    inference_fn(ckpt, flags.inference_input_file,
    trans_file, hparams, num_workers, jobid)
    
    # Evaluation
641: # Evaluation
    ref_file = flags.inference_ref_file
    if ref_file and tf.gfile.Exists(trans_file):
    for metric in hparams.metrics:
    score = evaluation_utils.evaluate(
    ref_file,
    trans_file,
    metric,
    hparams.subword_option)
    utils.print_out("  %s: %.1f" % (metric, score))
    else:
    # Train
652: # Train
    train_fn(hparams, target_session=target_session)
    
    
    def main(unused_argv):
    default_hparams = create_hparams(FLAGS)
    train_fn = train.train
    inference_fn = inference.inference
    run_main(FLAGS, default_hparams, train_fn, inference_fn)
    
    
    if __name__ == "__main__":
    nmt_parser = argparse.ArgumentParser()
    add_arguments(nmt_parser)
    FLAGS, unparsed = nmt_parser.parse_known_args()
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

